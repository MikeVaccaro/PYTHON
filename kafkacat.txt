
#CONSUME GET ALL FIELDS
PASSWORD=TArsy8XrhefW3WV8
USERNAME=dds-repl-dev
SECCLAUSE="-X security.protocol=SASL_PLAINTEXT -X sasl.mechanisms=SCRAM-SHA-256 -X sasl.username=$USERNAME -X sasl.password=$PASSWORD"
RUNCOMMAND="sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b $KBK"
FORMAT="-f 'Topic %t Partition [%p] at Offset %o: Timesestamp is: %T Payload size is %S  Key is %k   Message is: %s\n' "
RUNCOMMAND=$RUNCOMMAND" "$SECCLAUSE 




sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b $KBK $SECCLAUSE $OPERATIONCLAUSE

always 
-t topic

#Metadata listing:
-L 

#QUERY a certain T:P:TS and return its offset
-Q -t mikev2:0:1623957181214
-Q -t test1:1:1635507560823
#CONSUME
Consumer mode (writes messages to stdout):
-C 
-C -o beginning
-C -o end
-C -p 0
-C -p 1 -o 11514
-C -f 'Topic %t Partition [%p] at Offset %o: Timesestamp is: %T Payload size is %S  Key is %k   Message is: %s\n' 


#FORMATS
-f 'Topic %t Partition [%p] at Offset %o: Timesestamp is: %T Payload size is %S Message is: %s\n' 
-f '' -J

Format string tokens:
  %s                 Message payload
  %S                 Message payload length (or -1 for NULL)
  %R                 Message payload length (or -1 for NULL) serialized
                     as a binary big endian 32-bit signed integer
  %k                 Message key
  %K                 Message key length (or -1 for NULL)
  %T                 Message timestamp (milliseconds since epoch UTC)
  %h                 Message headers (n=v CSV)
  %t                 Topic
  %p                 Partition
  %o                 Message offset
  \n \r \t           Newlines, tab
  \xXX \xNNN         Any ASCII character




#AVRO
sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b kafkaincl01b1.chrobinson.com:9092 -C -t execution-tasknotificationevent -r http://kafkainschemacl01.chrobinson.com:8081 -s value=avro -s key=avro -p 0 -o 1800000 -f 'Key is %k %o  \n'
sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b kafkaprcl01b1.chrobinson.com:9092 -C -t execution-tasknotificationevent -r http://kafkaprschemacl01.chrobinson.com:8081  -s key=avro -p 0 -o beginning -f 'Timestamp and Payload are: %T  %s %o  \n' -c 1 > jj

get first offset for partition
sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b rh01pr-kf1b1.centralus.chrazure.cloud  -C -t shipmenttracking-tracking-message-event -r http://kafkaprschemacl01.chrobinson.com:8081  -s key=avro -p 1 -o beginning -f 'Timestamp and Payload are: %p  %o  \n' -c 1



sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b kafkadvcl01b1.chrobinson.com:9092 -L -t  mikev2
	Metadata for mikev2 (from broker -1: kafkadvcl01b1.chrobinson.com:9092/bootstrap):
 	8 brokers:
 	 	broker 22955 at lin2dv2kf1b16.chrobinson.com:9092
 	 	broker 1005 at lin2dv2kf1b12.chrobinson.com:9092
  		broker 1003 at lin2dv2kf1b10.chrobinson.com:9092
  		broker 1004 at lin2dv2kf1b11.chrobinson.com:9092
  		broker 1007 at lin2dv2kf1b14.chrobinson.com:9092 (controller)
 		broker 1001 at lin2dv2kf1b8.chrobinson.com:9092
  		broker 1006 at lin2dv2kf1b13.chrobinson.com:9092
  		broker 1002 at lin2dv2kf1b9.chrobinson.com:9092
 	1 topics:
  	topic "mikev2" with 3 partitions:
    		partition 0, leader 1007, replicas: 1007,1002, isrs: 1002,1007
    		partition 1, leader 22955, replicas: 22955,1003, isrs: 22955,1003
    		partition 2, leader 1001, replicas: 1001,1004, isrs: 1004,1001


sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b kafkadvcl01b1.chrobinson.com:9092 -X list 
	## Global configuration properties

	Property                                 | C/P | Range           |       Default | Importance | Description
	-----------------------------------------|-----|-----------------|--------------:|------------| --------------------------
	client.id                                |  *  |                 |       rdkafka | low        | Client identifier. <br>*Type: string*
	message.max.bytes                        |  *  | 1000 .. 1000000000 |       1000000 | medium     | Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation). <br>*Type: integer*
	message.copy.max.bytes                   |  *  | 0 .. 1000000000 |         65535 | low        | Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs. <br>*Type: integer*
	receive.message.max.bytes                |  *  | 1000 .. 2147483647 |     100000000 | medium     | Maximum Kafka protocol respon
	

sudo docker run -it --rm edenhill/kafkacat:1.6.0 -b kafkadvcl01b1.chrobinson.com:9092 -X dump
	# Global config
	builtin.features = gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer
	client.id = rdkafka
	client.software.name = librdkafka
	message.max.bytes = 1000000
	message.copy.max.bytes = 65535
	receive.message.max.bytes = 100000000
	max.in.flight.requests.per.connection = 1000000
	metadata.request.timeout.ms = 60000
	topic.metadata.refresh.interval.ms = 300000
	metadata.max.age.ms = 900000
	topic.metadata.refresh.fast.interval.ms = 250
	topic.metadata.refresh.fast.cnt = 10
	topic.metadata.refresh.sparse = true
	topic.metadata.propagation.max.ms = 30000




